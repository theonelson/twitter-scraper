{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            -----            Twitter scraper with snscrape            -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about scraping tweets from Twitter with key words and storing them in clean csv datasets, in order to used for data analysis/machine learning. It works with the webscraper 'snscrape', without the official Twitter API.\n",
    "\n",
    "The created datasets contain the following columns: \n",
    "\n",
    "- date (datetime)\n",
    "- text (str)\n",
    "- number of retweets (float)\n",
    "- number of likes (float)\n",
    "- number of views (float) (since december 2022)\n",
    "- language (str)\n",
    "- username (str)\n",
    "- number of followers of the user (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install snscrape\n",
    "#pip install pandas\n",
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm # make your loops show a progress meter\n",
    "import itertools               #library for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query(company, start_date, end_date):\n",
    "    \n",
    "    #add a function for compound names (example : 'Arcelor_Mittal' -> 'Areclor_Mittal' + 'ArcelorMittal')\n",
    "    words = company.split(\"_\")\n",
    "    if len(words) > 1:\n",
    "        company = '\"' + company + '\"  OR ' + \"\".join(words)\n",
    "    else:\n",
    "        company = company\n",
    "    \n",
    "    return f\"{company} since:{start_date} until:{end_date}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting a list of objects 'tweets' with the snscrape function and putting them in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, maxTweets):\n",
    "    \n",
    "    df = pd.DataFrame(tqdm(itertools.islice(sntwitter.TwitterSearchScraper(query).get_items(),maxTweets), total=maxTweets))\n",
    "    df = df[['date', 'user','renderedContent', 'retweetCount', 'likeCount', 'viewCount','lang']]\n",
    "    df.rename(columns = {'retweetCount':'retweets', 'likeCount':'likes','viewCount':'views', 'renderedContent':'text'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing the links and the @ with regex. Also removing the useless spaces and empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning(df):    \n",
    "    \n",
    "    regex_links_arobase = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))|@\\w+|(https?:\\/\\/)?(www\\.)?[a-z0-9-]+\\.(com|org)(\\.[a-z]{2,3})?\"\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(regex_links_arobase, ' ', x).strip())\n",
    "    \n",
    "    regex_space = r'\\s+'\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(regex_space, ' ', x).strip())\n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\n{2,}', '\\n', x))\n",
    "    \n",
    "    df = df.dropna(subset=['text'], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    df = df.astype({\"text\": 'str'})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the followers and username column from the information in the user column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_username_and_followers(df):\n",
    "    \n",
    "    usernames = []\n",
    "    followers = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        user_data = row['user']\n",
    "        \n",
    "        usernames.append(user_data['username'])\n",
    "        followers.append(user_data['followersCount'])\n",
    "    \n",
    "    df['username'] = usernames\n",
    "    df['followers'] = followers\n",
    "    \n",
    "    df.drop('user', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving tweets in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_to_csv(name, df):\n",
    "    # create 'tweets' folder if it doesn't exist\n",
    "    if not os.path.exists(\"tweets\"):\n",
    "        os.makedirs(\"tweets\")\n",
    "    \n",
    "    name=name.lower()\n",
    "    folder_path = \"tweets\"\n",
    "    \n",
    "    #Add a number after the name if it already exists\n",
    "    i = 0\n",
    "    file_path = os.path.join(folder_path, f\"{name}_tweets.csv\")\n",
    "    while os.path.exists(file_path):\n",
    "        i += 1\n",
    "        file_path = os.path.join(folder_path, f\"{name}_tweets{i}.csv\")\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"The file {file_path} was successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main function : calling all the functions before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scraping(company, start_date, end_date, maxTweets):\n",
    "    \n",
    "    query = format_query(company, start_date, end_date)\n",
    "    df_raw = search_tweets(query, maxTweets)\n",
    "    df_clean = Cleaning(df_raw)\n",
    "    df_clean = df_clean.dropna(subset=['text'], axis=0).reset_index(drop=True)\n",
    "    df_final = add_username_and_followers(df_clean)\n",
    "    save_tweets_to_csv(company, df_final)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We fill the 4 following information : \n",
    "\n",
    "    - List of the names to search\n",
    "    - start date\n",
    "    - end date\n",
    "    - maximum number of tweets per name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = ['air_france', 'credit_agricole']\n",
    "\n",
    "start_date= \"2022-12-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "maxTweets = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aafb622fe1b4c9999ba1b2945e46dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progression :  :   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de02050129b45c7adf0e9c58322039c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file tweets\\air_france_tweets3.csv was successfully created.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aefd9601ebb48b3bad0ed4a02a43b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file tweets\\credit_agricole_tweets.csv was successfully created.\n"
     ]
    }
   ],
   "source": [
    "for comp in tqdm(comp_list, desc = \"Progression :  \"):\n",
    "    Scraping(comp, start_date, end_date, maxTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
